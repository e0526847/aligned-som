{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned SOM\n",
    "The *TensorFlow* implementation of the [Basic SOM](https://github.com/e0526847/basic-som/blob/master/Basic%20SOM.ipynb) can be expanded to an *Aligned SOM*, as it is introduced in the paper [Aligned Self-Organizing Maps](https://pdfs.semanticscholar.org/e75a/822a0cc98ab123db7f271ae5402ef3081835.pdf) by Elias Pampalk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Methods, variables and operations which have been introduced as part of the **Aligned SOM**, are marked with a **as_** prefix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # notebook basedtensorflow v. 1.4.0.  \n",
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class SOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **class SOM** represents a self-organizing map, and in terms of an *aligned SOM*, it represents a single **SOM layer**. It is defined with the following parameters:    \n",
    "-  **m, n**: &nbsp; dimensions of the SOM layer\n",
    "-  **dim**: &nbsp; number of item features    \n",
    "-  **layer**: &nbsp; number of the layer it represents (default: 0)\n",
    "-  **n_iterations**: &nbsp; total number of training iterations (default: 1)\n",
    "-  **alpha**: &nbsp; learning rate (default: 0.3)\n",
    "-  **sigma**: &nbsp; radius of the *best matching unit* (BMU), used as the neighbourhood value (default: max(m,n)/2.0)    \n",
    "\n",
    "A TensorFlow graph **_graph** contains the dataflow structure that is needed for the training procedure.    \n",
    "The initialization of a SOM layer is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOM(object):\n",
    "    def __init__(self, m, n, dim, layer=0, n_iterations=1, alpha=None, sigma=None):\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        self._dim = dim\n",
    "        if alpha is None:\n",
    "            self.alpha = 0.3\n",
    "        else:\n",
    "            self.alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            self.sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            self.sigma = float(sigma)\n",
    "        self.as_layer = layer\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    "        \n",
    "        self._graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class methods    \n",
    "The **SOM** class requires some methods we will introduce one after another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **_neuron_locations**    \n",
    "yields the 2-D locations of the individual neurons in the SOM layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _neuron_locations(self,m,n):\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            yield np.array([i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **as_train**    \n",
    "is a modified *train* method of the *Basic SOM*;    \n",
    "it trains the SOM layer for one iteration (no. *iter_no*) on one item (*input_vect*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_train(self, input_vect, iter_no):\n",
    "    self._sess.run(self._training_op, feed_dict={self._vect_input: input_vect, self._iter_input: iter_no})\n",
    "    self._weightages = list(self._sess.run(self._weightage_vects)) # list of weight vectors, starting with the upper left one\n",
    "    self._locations = list(self._sess.run(self._location_vects)) # list of the corresponding locations [x,y]\n",
    "    self._sess.run(self.as_delta_op, feed_dict={self._vect_input: input_vect, self._iter_input: iter_no}) # deltas from this training\n",
    "    self._trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **as_align**    \n",
    "depending on the *decay* rate and the *delta* values obtained during the training iteration of the current layer, the method aligns all remaining layers;    \n",
    "the higher *decay* the stronger is the influence of the results from the current layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_align(self, deltas, decay):\n",
    "    with self._graph.as_default():\n",
    "        as_align_op = tf.assign(self._weightage_vects, tf.add(self._weightage_vects, tf.multiply(tf.convert_to_tensor(deltas), decay)))\n",
    "    self._sess.run(as_align_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **map_vects**    \n",
    "maps each input vector to the relevant neuron in the SOM grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vects(self, input_vects):\n",
    "    if not self._trained:\n",
    "        raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "    to_return = []\n",
    "    for vect in input_vects:\n",
    "        min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect - self._weightages[x]))\n",
    "        to_return.append(self._locations[min_index])\n",
    " \n",
    "    return to_return    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **init_graph**    \n",
    "The method **init_graph()** initializes the previously created graph with its operations and tensors;    \n",
    "<font color=grey>the initialization was originally part of the class costructor. For the purposes of a step by step execution, it is split and moved here.</font>\n",
    "\n",
    "> The following data is stored in the <font color=green>class SOM</font>:\n",
    "-  **_weightage_vects**: weight vectors of the units, each of dimentionality *dim*, stored in a matrix of the size [m*n,dim]    \n",
    "-  **as_deltas**: delta values from the training iteration\n",
    "-  **as_p**: vector containing the layer's p-values\n",
    "-  **_location_vects**: SOM grid location of the units, stored as coordinates x,y in a matrix of the size [m*n, 2]\n",
    "-  **_vect_input**: training vector\n",
    "-  **_iter_input**: iteration number\n",
    "\n",
    "> The following data is calculated during the <font color=green>training</font>:    \n",
    "-  **as_bmu_index**: index of the *best matching unit*, based on the Euclidean distance between the unit's weight vector and the input, includes the layer's p-values\n",
    "-  **bmu_loc**: location of the *best matching unit*, based on its index\n",
    "-  **learning_rate_op**: learning rate operation\n",
    "-  **_alpha_op**: learning rate, decreasing with each iteration\n",
    "-  **_sigma_op**: neighbourhood radius, decreasing with each iteration\n",
    "-  **bmu_distance_squares** Manhattan distance from the BMU\n",
    "-  **neighbourhood_func** neighbourhood operation: $e^{(-bmu\\_dist/sigma)}$\n",
    "<a id='init_graph'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph(self):\n",
    "    self._graph = tf.Graph()\n",
    "    with self._graph.as_default(): # declare as the default graph of the current thread\n",
    "        self._weightage_vects = tf.Variable(as_weights) # initial weight vectors with random values (identical for each layer)\n",
    "        self.as_deltas = tf.Variable(tf.zeros([self._m*self._n,self._dim])) # delta values from the training, initialized with 0.0\n",
    "        self.as_p = tf.concat([tf.tile(tf.constant([as_p1[self.as_layer]]), [as_f1]), \n",
    "                               tf.tile(tf.constant([as_p0[self.as_layer]]), [as_f2])], 0) # p-vector for this layer\n",
    "        self._location_vects = tf.constant(np.array(list(self._neuron_locations(self._m, self._n)))) # SOM grid location\n",
    "        self._vect_input = tf.placeholder(\"float\", [self._dim]) # training vector\n",
    "        self._iter_input = tf.placeholder(\"float\") # iteration number\n",
    "        \n",
    "        # operations of the training procedure...\n",
    "        # BMU index\n",
    "        as_bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.multiply(tf.stack(\n",
    "                    [self._vect_input for i in range(self._m*self._n)]), self.as_p)), 2), 1)), 0)\n",
    "        \n",
    "        # BMU location as [x y]\n",
    "        slice_input = tf.pad(tf.reshape(as_bmu_index, [1]), np.array([[0,1]]))\n",
    "        bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,tf.constant(np.array([1, 2]))),[2])\n",
    "        \n",
    "        # learning rate\n",
    "        learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input, self._n_iterations)) # 1-(current iteration / number iterations)\n",
    "        _alpha_op = tf.multiply(self.alpha, learning_rate_op) # default alpha * decreasing learning rate\n",
    "        _sigma_op = tf.multiply(self.sigma, learning_rate_op) # default sigma * decreasing learning rate\n",
    "        \n",
    "        # neighbourhood\n",
    "        bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(self._location_vects, tf.stack([bmu_loc for i in range(self._m*self._n)])), 2), 1)\n",
    "        neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "        \n",
    "        # operations for the update on weight vectors\n",
    "        # new weight = old weight + delta, where delta = lrm * (input vector - current weight vector)\n",
    "        learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    "        learning_rate_multiplier = tf.stack([tf.tile(tf.slice(learning_rate_op, np.array([i]), np.array([1])), [self._dim]) for i in range(self._m*self._n)])\n",
    "        self.weightage_delta = tf.multiply(learning_rate_multiplier,\n",
    "                            tf.subtract(tf.stack([self._vect_input for i in range(self._m*self._n)]),self._weightage_vects))                                         \n",
    "        new_weightages_op = tf.add(self._weightage_vects, self.weightage_delta)\n",
    "        self._training_op = tf.assign(self._weightage_vects, new_weightages_op) # update weight vectors (weight vector + delta)\n",
    "        self.as_delta_op = tf.assign(self.as_deltas, self.weightage_delta) # calculated delta values for the align process\n",
    "                \n",
    "        # session initialization\n",
    "        self._sess = tf.Session()\n",
    "        \n",
    "        # global variable initialization\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self._sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, due to a step-by-step execution, the described methods are added to each **SOM** layer retrospectively (see [example](#example_animals) below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions  \n",
    "In order to coordinate and align the layers of the *aligned SOM*, some general functions are required:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **as_init_p**    \n",
    "two distinct p-values relate to each SOM layer, indicating the influence of the particular aspects of similarity,    \n",
    "where the focus is on the first aspect on the first layer, and on the second aspect on the final layer;    \n",
    "for all the layers in between, the p-values are interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values\n",
    "def as_init_p(l):\n",
    "    # layers 0 to (<number layers>-1)\n",
    "    as_layers = tf.range(0, l, delta=1, dtype=tf.float32)\n",
    "    # scaling to [0,1] --> tensor with p-values corresponding to the layers in <as_layers>, starting with p=0\n",
    "    as_p0 = tf.div(tf.subtract(as_layers,tf.reduce_min(as_layers)),\n",
    "    tf.subtract(tf.reduce_max(as_layers),tf.reduce_min(as_layers)))\n",
    "    # p0 in reversed order --> tensor with p-values corresponding to the layers in <as_layers>, starting with p=1\n",
    "    as_p1 = tf.keras.backend.reverse(as_p0, 0)\n",
    "    return as_p0, as_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **as_init_weights**    \n",
    "initialization of weight vectors, identical for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial weight vectors (identical for all layers)\n",
    "def as_init_weights(m,n,dim):\n",
    "    return tf.random_normal([m*n, dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **as_align_layers**    \n",
    "due to mutual influence, all layers are adapted to the results of the current *layer* after its training iteration;    \n",
    "the closer a layer is to the current layer the stronger it is adjusted;\n",
    "*coef* describes the strength of the general influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights adjustment between layers\n",
    "def as_align_layers(layer, coef=1.0):\n",
    "    if (layer.as_layer > 0):\n",
    "        for i in range(0, layer.as_layer):\n",
    "            decay = coef*(2**(-(layer.as_layer-i)))\n",
    "            as_layers[i].as_align(layer._sess.run(layer.as_deltas), decay)\n",
    "    if (layer.as_layer < as_l-1):\n",
    "        for j in range(layer.as_layer+1, as_l):\n",
    "            decay = coef*(2**(-(j-layer.as_layer)))\n",
    "            as_layers[j].as_align(layer._sess.run(layer.as_deltas), decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example_animals'></a>\n",
    "## Example - Animal clustering:    \n",
    "The closer a layer number is to 0 the stronger it shows the aspect of the animal appearance. On the other hand, the closer a layer number is to the total number of layers the stronger it shows the aspect of the animal behaviour.    \n",
    "\n",
    "According to the following encoding of animal features, all the birds, except for the *eagle*, have identical appearance. Hence, the first layer (layer 0) of the aligned SOM, which considers the nine appearance features foremost, should cluster them together. With an increasing layer number though, the *eagle* should close in on the *hawk* and the *owl* since they share the same behaviour.    \n",
    "\n",
    "*owl* and *hawk* are encoded as having identical appearance as well as identical behaviour. Hence we should see the two of them close together on each of the layers. The same goes for *horse* and *zebra*.\n",
    "\n",
    "Please keep in mind that the layers influence each other. With each iteration, a random layer is trained on a random animal, so depending on the order of trained layers and the particular animal, the final clustering can turn out slightly different each time. \n",
    "\n",
    "The SOM mapping of the animals can be found at the end of the following code cell. The test includes five layers and 200 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types # for the retrospective inclusion of methods to a class instance\n",
    "from matplotlib import pyplot as plt # for plotting the images\n",
    "%matplotlib inline\n",
    "\n",
    "# one-hot encoding of animal features\n",
    "# the first 9 values belong to the view aspect of appearance, followed by the animal's behaviour\n",
    "animals = np.array(\n",
    "     [[1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.], # dove\n",
    "      [1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # hen\n",
    "      [1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.], # duck\n",
    "      [1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.], # goose\n",
    "      [1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.], # owl\n",
    "      [1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.], # hawk\n",
    "      [0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.], # eagle\n",
    "      [0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.], # fox\n",
    "      [0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.], # dog\n",
    "      [0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.], # wolf\n",
    "      [1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.], # cat\n",
    "      [0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.], # tiger\n",
    "      [0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.], # lion\n",
    "      [0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.], # horse\n",
    "      [0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.], # zebra\n",
    "      [0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.]])# cow\n",
    "animals_names = \\\n",
    "    ['dove', 'hen', 'duck', 'goose', 'owl', 'hawk', 'eagle', 'fox', \n",
    "     'dog', 'wolf', 'cat', 'tiger', 'lion', 'horse', 'zebra', 'cow']\n",
    "\n",
    "main_sess = tf.Session()\n",
    "as_l = 5 # number of layers\n",
    "as_m = 20 # layer width\n",
    "as_n = 30 # layer height\n",
    "as_dim = 13 # item dimentionality (features)\n",
    "as_f1 = 9 # number features of aspect 1 (appearance)\n",
    "as_f2 = as_dim - as_f1 # number features of aspect 2 (activity)\n",
    "as_iter = 200 # number of iterations\n",
    "as_infl = 1.0 # intensity of mutual influence between layers\n",
    "as_layers = [] # layers of the aligned SOM\n",
    "as_mapped = [] # item mapping to neurons\n",
    "\n",
    "# init p values\n",
    "as_p0, as_p1 = main_sess.run(as_init_p(as_l))\n",
    "# init weight vectors (identical for all layers)\n",
    "as_weights = main_sess.run(as_init_weights(as_m,as_n,as_dim))\n",
    "\n",
    "# init layers\n",
    "for i in range(as_l):\n",
    "    as_layers.append(SOM(as_m, as_n, as_dim, i, as_iter))\n",
    "    # add class methods to the SOM objects/layers\n",
    "    as_layers[i]._neuron_locations = types.MethodType(_neuron_locations,as_layers[i])\n",
    "    as_layers[i].as_train = types.MethodType(as_train,as_layers[i])\n",
    "    as_layers[i].as_align = types.MethodType(as_align,as_layers[i])\n",
    "    as_layers[i].map_vects = types.MethodType(map_vects,as_layers[i])\n",
    "    as_layers[i].init_graph = types.MethodType(init_graph,as_layers[i])\n",
    "    as_layers[i].init_graph()\n",
    "\n",
    "#------------------------ main training procedure\n",
    "for i in range(as_iter):\n",
    "    as_next_layer = randint(0, as_l-1) # select a random layer\n",
    "    as_next_item = randint(0,len(animals)-1) # select a random item\n",
    "    # train current layer on current item\n",
    "    as_layers[as_next_layer].as_train(animals[as_next_item],i)\n",
    "    # align remaining layers\n",
    "    as_align_layers(as_layers[as_next_layer], as_infl)\n",
    " \n",
    "# map animals to their closest neurons\n",
    "for i in range(as_l):\n",
    "    as_mapped.append(as_layers[i].map_vects(animals))\n",
    "\n",
    "#------------------------ plots\n",
    "as_plots = [0,1,2,3,4] # layers to be plotted\n",
    "\n",
    "for i in as_plots:\n",
    "    plt.axis([-5, as_m+5, -10, as_n+5])\n",
    "    taken_cells = []\n",
    "    plt.title('Animal SOM, Layer ' + str(i) + ' (' + str(as_layers[i]._m) + ' x ' + str(as_layers[i]._n) + '), ' + str(as_iter) + ' iterations')\n",
    "    for i, m in enumerate(as_mapped[i]):\n",
    "        for c in taken_cells:\n",
    "            if (c==m).any():\n",
    "                m[1] = m[1]-2\n",
    "        taken_cells.append(m)\n",
    "        plt.text(m[0], m[1], animals_names[i], fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "main_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='alert alert-block alert-info'>188.413 *Self-organizing Systems*, TU Wien, WS2017, GR05</div></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
